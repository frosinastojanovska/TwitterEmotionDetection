{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion detection from tweets - tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from feature_selection import feature_selection\n",
    "from preprocessing import fix_encoding, split_tweet_sentences, tokenize_tweets, get_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and preprocess dataset\n",
    "\n",
    "Load the dataset contaning the tweets and their emotions. For text preprocessing first the tweet text is encoded (fixed), then the tweet is tokenized, and finally the tokens are lematized. The dataset needs to have a column with name tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id    class                                             tokens  \\\n",
      "0  1000001  neutral  [[Check, this, video, out, -, -, President, Ob...   \n",
      "1  1000002  neutral  [[need, suggestions, for, a, good, IR, filter,...   \n",
      "2  1000003  neutral  [[@surfit, :, I, just, checked, my, google, fo...   \n",
      "3  1000004  neutral  [[is, in, San, Francisco, at, Bay, to, Breaker...   \n",
      "4  1000005  neutral               [[just, landed, at, San, Francisco]]   \n",
      "5  1000006  neutral  [[San, Francisco, today, .], [Any, suggestions...   \n",
      "6  1000007  neutral  [[On, my, way, to, see, Star, Trek, @, The, Es...   \n",
      "7  1000008  neutral  [[Going, to, see, star, trek, soon, with, my, ...   \n",
      "8  1000009  neutral  [[Bill, Simmons, in, conversation, with, Malco...   \n",
      "9  1000010  neutral    [[playing, with, cURL, and, the, Twitter, API]]   \n",
      "\n",
      "                                              lemmas  \n",
      "0  [[Check, this, video, out, -, -, President, Ob...  \n",
      "1  [[need, suggestion, for, a, good, IR, filter, ...  \n",
      "2  [[@user, :, I, just, check, my, google, for, m...  \n",
      "3  [[be, in, San, Francisco, at, Bay, to, Breaker...  \n",
      "4                 [[just, land, at, San, Francisco]]  \n",
      "5  [[San, Francisco, today, .], [Any, suggestion,...  \n",
      "6  [[On, my, way, to, see, Star, Trek, @, The, Es...  \n",
      "7  [[Going, to, see, star, trek, soon, with, my, ...  \n",
      "8  [[Bill, Simmons, in, conversation, with, Malco...  \n",
      "9       [[play, with, cURL, and, the, Twitter, API]]  \n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_excel('data/merged_datasets.xlsx')\n",
    "# fix the tweet text\n",
    "dataset = fix_encoding(dataset)\n",
    "# split the tweet text into sentences\n",
    "dataset = split_tweet_sentences(dataset)\n",
    "# tokenize each sentence of the tweets\n",
    "dataset = tokenize_tweets(dataset)\n",
    "# lemmatise the tweets\n",
    "dataset = get_lemmas(dataset)\n",
    "\n",
    "dataset = dataset.drop(['emotion_intensity', 'tweet'], axis=1)\n",
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id    class                                             lemmas  \\\n",
      "0  1000001  neutral  [Check, this, video, out, -, -, President, Oba...   \n",
      "1  1000002  neutral  [need, suggestion, for, a, good, IR, filter, f...   \n",
      "2  1000003  neutral  [@user, :, I, just, check, my, google, for, my...   \n",
      "3  1000004  neutral  [be, in, San, Francisco, at, Bay, to, Breakers...   \n",
      "4  1000005  neutral                   [just, land, at, San, Francisco]   \n",
      "5  1000006  neutral     [San, Francisco, today, ., Any, suggestion, ?]   \n",
      "6  1000007  neutral  [On, my, way, to, see, Star, Trek, @, The, Esq...   \n",
      "7  1000008  neutral  [Going, to, see, star, trek, soon, with, my, d...   \n",
      "8  1000009  neutral  [Bill, Simmons, in, conversation, with, Malcol...   \n",
      "9  1000010  neutral         [play, with, cURL, and, the, Twitter, API]   \n",
      "\n",
      "                                            valences  \n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "9  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n"
     ]
    }
   ],
   "source": [
    "vocab = [item.lower() for lemmas in dataset.lemmas.values for item in list(chain(*lemmas))]\n",
    "vocab = np.unique(vocab)\n",
    "# dictionary of every word in the vocabulary with its index\n",
    "dictionary = dict()\n",
    "for i in range(len(vocab)):\n",
    "    dictionary[vocab[i]] = i\n",
    "\n",
    "# create initial feature for classification\n",
    "featured_dataset = dataset[['id', 'class', 'lemmas']]\n",
    "featured_dataset['valences'] = ''\n",
    "for index, row in dataset.iterrows():\n",
    "    lemmas = list(chain(*row.lemmas))\n",
    "    featured_dataset.set_value(index=index, col='lemmas', value=lemmas)\n",
    "    valences = [1] * len(lemmas)\n",
    "    featured_dataset.set_value(index=index, col='valences', value=valences)\n",
    "    # 0 is the value when the word is not present\n",
    "    initial_valences = np.zeros(len(vocab))\n",
    "    for lemma, valence in zip(lemmas, valences):\n",
    "        initial_valences[dictionary[lemma.lower()]] = valence\n",
    "    featured_dataset.set_value(index=index, col='valences', value=initial_valences)\n",
    "    \n",
    "print(featured_dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_selection\n",
      "[('happy', 0.012420160847448316), ('smile', 0.012400006270757793), ('nervous', 0.010819843777147073), ('sad', 0.010372171043425396), ('bitter', 0.009548802876973725), ('depression', 0.00934140622451959), ('hilarious', 0.008282177557744696), ('nightmare', 0.007844092835140917), ('rage', 0.007489818740668494), ('laughter', 0.0074166201571720545), ('panic', 0.0072438030206095055), ('terrorism', 0.007221643206650254), ('angry', 0.007216668028348058), ('revenge', 0.007016176816593769), ('anxiety', 0.006868576023274911), ('bully', 0.006685358814951264), ('unhappy', 0.006558597288690772), ('offend', 0.006509898152800927), ('optimism', 0.006413389736878103), ('sadness', 0.00630535784423931), ('fear', 0.00629811433904764), ('rejoice', 0.005901269943717621), ('cheer', 0.005897968471765254), ('outrage', 0.005855350256562923), ('terrible', 0.005648353344638163), ('terror', 0.005528074393716715), ('anger', 0.005343222679556044), ('lost', 0.005128141561090122), ('depressing', 0.005098858310419312), ('depress', 0.005094377992881647), ('grim', 0.005057018196258183), ('furious', 0.004996737695646844), ('horrible', 0.004953990616400032), ('sober', 0.00481609651186512), ('awful', 0.004730032895303926), ('lively', 0.004727498650562903), ('fuming', 0.004724067668887647), ('horror', 0.004723222866977654), ('dark', 0.0046217869410194274), ('hilarity', 0.004581399622997461), ('sadly', 0.0043605776903277585), ('@user', 0.0043460629474667625), ('delight', 0.004288785653820413), ('.', 0.00419798956651448), ('shocking', 0.0040599280616672), ('glee', 0.0038150331421642147), ('fume', 0.0037784972193537873), ('joyful', 0.00374607125335122), ('sink', 0.0036929571195893386), ('fury', 0.003658215231438186), ('!', 0.003577067563038384), ('broadcast', 0.003575011368009866), ('the', 0.003492914028063726), ('elate', 0.003440495636026585), ('a', 0.003436515300296028), ('alarm', 0.0032207272706778244), ('afraid', 0.0032062073548772855), ('be', 0.003178829265915196), ('i', 0.003177698561976138), ('boil', 0.0031277113465702396), ('shake', 0.003116078298528796), ('discourage', 0.003097244800480612), ('insult', 0.003091218044670739), ('breezy', 0.003087354494030581), ('and', 0.0030707709345100523), ('serious', 0.0029835396954890577), ('snap', 0.0029411986000575973), ('shy', 0.0029259874627975096), ('gloomy', 0.0029190384406980944), ('joyous', 0.0028585094626145304), ('to', 0.002821303687061652), ('of', 0.0028066541438215207), ('have', 0.0028065653139591673), ('blue', 0.0028038455456229775), ('rag', 0.0027151589314147297), ('bright', 0.0026931708322455055), ('burn', 0.0026893245646864078), (',', 0.002677805700668332), ('exhilarate', 0.0026418087425101157), ('dull', 0.002632141430367573), ('hearty', 0.0025827601374203554), ('that', 0.0025774898674517503), ('you', 0.002568714432521816), ('jquery', 0.002567994526909046), ('in', 0.0025124619903159357), ('despair', 0.002403207146593435), ('gloom', 0.002349188807825668), ('wrath', 0.0023137244037757685), ('my', 0.002302863746963625), ('rabid', 0.0023007706005003874), ('live.ly', 0.0022987446745696563), ('irritate', 0.0022911667880608612), ('museum', 0.002289711264434999), ('cheerful', 0.0022884447144393754), ('musically', 0.0022859184729747684), ('resent', 0.0022731034458579043), ('this', 0.0022632020327136714), ('love', 0.0022316360178574655), ('offense', 0.002228815392549064), ('pout', 0.0022093814937281455), ('?', 0.002191703818311593), ('for', 0.002189961079311236), ('dreadful', 0.002180897185384265), ('just', 0.002177592090382612), ('safeway', 0.002164248805974938), ('not', 0.002135366209732164), ('me', 0.002120641172695599), ('horrific', 0.002106799773125432), ('people', 0.0020924869093184436), ('cheerfully', 0.002070801094808804), ('on', 0.002037415372190242), ('mourn', 0.0020371712680864575), ('smiling', 0.002005010289165959), ('playful', 0.002002053129139766), ('offended', 0.001966171509699165), ('so', 0.0019567420187947973), ('do', 0.0019329766453594812), ('get', 0.0019006680428485107), (\"it's\", 0.0018599275734121484), ('it', 0.0018528825847473219), ('sulk', 0.0018480715094932917), ('worry', 0.0018330836859309634), ('dismal', 0.0018279846023628436), ('with', 0.0018106672449316606), ('restless', 0.0017751021848495592), ('lose', 0.0017677269617317754), ('sting', 0.0017667219578816104), (\"i'm\", 0.0017450950972003356), ('all', 0.00174128019177077), ('shock', 0.001700125624236866), ('burst', 0.0016715851109095324), ('madden', 0.0016651605476464381), ('up', 0.0016582420068876956), ('good', 0.0016427080602685732), ('dread', 0.001638808694728432), ('grudge', 0.0016387379762388258), ('cheery', 0.0016084044859629638), ('sunk', 0.0016060838241773883), ('but', 0.001594439331816424), ('chirp', 0.00158255121570866), ('if', 0.0015811019439031133), ('at', 0.0015784817025949812), ('weary', 0.0015528826115229154), ('relentless', 0.0015450187569485496), ('amazing', 0.0015251623502693659), ('frown', 0.0015211413531066808), ('or', 0.0015122896072481855), ('what', 0.001496023082206441), ('gm', 0.0014890079158777034), ('threaten', 0.0014836268496079242), ('stay', 0.0014561676121153833), ('go', 0.0014548793733305736), ('exhilaration', 0.0014521479573287072), ('-', 0.0014403026694311944), ('make', 0.001432401888347189), (\"'\", 0.001426745892042573), ('heyday', 0.001414497693375577), ('provoke', 0.001387782729236982), ('when', 0.0013720225475349957), ('now', 0.0013545703086864618), ('nike', 0.0013423001277136424), (':', 0.0013358048253192063), ('levity', 0.0013325550164168388), ('concern', 0.001332306126044811), ('huff', 0.0013284441017960788), ('day', 0.0013185121109883537), (\"can't\", 0.0013159967890997086), ('no', 0.0012813891132263623), ('victory', 0.0012547600685865), ('about', 0.0012501763688337003), ('&', 0.0012370995936369527), ('scare', 0.0012167656450174232), ('how', 0.0012121515282732002), ('feel', 0.0012120655313665643), ('bad', 0.0012046477518417834), ('eat', 0.0011899988960601183), ('beautiful', 0.0011869896977344523), ('please', 0.0011787100380154219), ('pakistan', 0.0011688109259548897), ('horrid', 0.0011503773330851352), ('great', 0.0011405301878879425), ('like', 0.0011340396742003816), ('your', 0.0011146118480785615), ('by', 0.0011097529232759929), ('wonderful', 0.0011077723391522587), ('an', 0.0011061522343350154), ('jovial', 0.001087460912571784), ('fuck', 0.0010829161468073997), ('today', 0.0010761470852506398), (':)', 0.0010748930232434412), ('baseball', 0.001068584397570295), ('cheering', 0.001042474502241202), ('why', 0.001030616772328745), ('he', 0.00102535100081133), ('never', 0.0010225341199660023), ('look', 0.0010138992518605287), ('as', 0.001002920490329129), (\"don't\", 0.000998366427265658), ('always', 0.0009981956702978054), ('want', 0.0009890883720546498), ('time', 0.0009869064261839142), ('know', 0.000983081342163836), ('her', 0.0009829979414553002), ('they', 0.0009746067880920618), ('out', 0.0009636602878014932), ('give', 0.0009571494292262255), ('mirth', 0.0009523851163872145), ('we', 0.0009447818144881366), ('start', 0.0009444977750750611), ('haunt', 0.0009364791089241518), ('see', 0.0009331465911340304), ('more', 0.0009315328470109848), ('really', 0.0009312730999701797), ('üòÇ', 0.0009246516712656595), ('too', 0.0009119739392339759), ('end', 0.0009071772201058849), ('resentment', 0.0009038342006756031), ('will', 0.0009015023182054008), ('his', 0.0008994746210437742), ('should', 0.0008980245860573287), ('still', 0.0008900732865642811), ('..', 0.0008887686458702984), ('lol', 0.0008884749985127394), ('there', 0.0008849964403690319), ('irate', 0.0008843741451019713), ('even', 0.0008842085219825412), ('over', 0.0008767810802824347), ('from', 0.0008763851589345089), ('exhilarating', 0.0008753511785677791), ('much', 0.0008751544220225221), ('tell', 0.0008746329390550845), ('hold', 0.0008656736215213545), ('animate', 0.000861823088132974), ('animated', 0.0008573320556168275), ('think', 0.0008554562985850238), ('can', 0.0008452753798419358), ('show', 0.0008450657596371467), ('sparkle', 0.000841173828973712), ('back', 0.0008405697663957177), ('attack', 0.0008403951101306192), ('fiery', 0.0008297704708724206), ('grieve', 0.0008244952585813953), ('api', 0.0008078955426555357), ('face', 0.0008073646762608178), ('irritation', 0.0008038938403352337), ('life', 0.0007880974734368483), ('pine', 0.000788078009842666), ('one', 0.0007761582327842451), ('san', 0.0007728997454693517), ('Ô∏è', 0.0007710237860189505), ('way', 0.0007708168333178024), ('let', 0.0007696154746944799), ('then', 0.0007620611379894634), ('trump', 0.0007593167288866388), ('could', 0.0007432447637098638), ('watch', 0.0007412204287069149), ('sparkling', 0.0007391853981777072), ('who', 0.0007374927958488109), ('dash', 0.0007363390900303462), ('twitter', 0.0007362523415606296), ('shudder', 0.000727083787416692), ('burning', 0.0007236234436513768), ('well', 0.0007123267961268746), ('birthday', 0.0006985571823072187), ('need', 0.0006980536034481613), ('off', 0.0006960739730591685), ('tantrum', 0.0006916208613791247), ('would', 0.000691150684412034), ('take', 0.000690517897746994), ('funny', 0.0006881636386955415), ('guy', 0.000683781830343127), ('tiff', 0.0006835705143229928), ('say', 0.000678308221946688), ('our', 0.0006757237042898084), ('right', 0.0006734152580778238), ('pessimist', 0.0006731646986067916), ('leave', 0.0006662942606433728), ('only', 0.0006644338716091134), ('u', 0.0006636732485081153), ('üò§', 0.0006584038915684032), ('some', 0.0006573322591543267), ('morning', 0.0006531808624421235), ('come', 0.0006501264946081703), ('im', 0.0006462740247487125), ('infuriate', 0.00064524506524772), ('someone', 0.0006442283925058641), ('2', 0.0006441226150297073), ('work', 0.000642647110468076), ('üòç', 0.0006413518045803391), ('ever', 0.000641316500627183), ('thank', 0.0006322500365460937), ('ugh', 0.0006314471816919504), ('mean', 0.000630228557613281), ('them', 0.0006272323912102548), ('new', 0.000618897689456775), (\"that's\", 0.0006180641228883675), ('blog', 0.0006169342621698039), ('thing', 0.0006147793211452431), ('kill', 0.0006121697846033178), ('night', 0.0006093113343873197), ('try', 0.0006064469751160455), ('any', 0.0006058992440697298), ('provocation', 0.0006038892769434177), (\"didn't\", 0.0006034864122007261), ('thanks', 0.0006017699224737426), ('maybe', 0.0006009660700148541), ('mhchat', 0.0005991000711526003), ('stanford', 0.0005943693052720913), ('those', 0.0005914746617253193), ('little', 0.000589529459238169), ('call', 0.0005890970415078279), ('laugh', 0.0005890089150542573), ('man', 0.0005838633429153991), ('sleep', 0.000581386637035351), ('üôÑ', 0.0005806408127172651), ('shiver', 0.0005783813068173303), ('accept', 0.0005727143679248797), ('same', 0.0005685044246421951), ('unga', 0.0005648519335745112), ('happen', 0.000562241250232847), ('most', 0.0005531700194055154), ('stop', 0.0005511103795062778), ('oh', 0.0005493979897748648), ('china', 0.0005490020583256949), (')', 0.0005462811237442266), ('wake', 0.0005460785691680675), ('down', 0.0005430328267986683), ('happiness', 0.000539404540100591), ('francisco', 0.0005351224346847334), ('old', 0.0005325548048621893), ('she', 0.0005301050908002957), ('find', 0.00052511654145951), ('him', 0.0005219472171132979), ('üò©', 0.0005217934415935776), ('hope', 0.0005213129619869339), ('use', 0.0005187771375699906), ('next', 0.0005156483649595187), ('police', 0.0005126486830601491), ('because', 0.000509081981400158), ('after', 0.00050878884641976), ('month', 0.0005085651360302603), ('üò°', 0.0005065913562381995), ('dentist', 0.0005002905864921808), ('which', 0.0004944386255993643), ('tomorrow', 0.0004937425442242405), ('seriously', 0.0004926605010445095), ('lot', 0.0004907236925687834), ('week', 0.0004887953267248217), ('something', 0.0004861463854256993), ('than', 0.00048563069264649693), ('head', 0.0004829804348773389), ('again', 0.0004815542125051266), ('hate', 0.0004793925190769539), ('other', 0.0004789683774997222), ('black', 0.0004734013999950033), ('üò≠', 0.00047054070537106573), (\"i've\", 0.00047000576105075617), ('through', 0.00046483192902948433), ('blood', 0.00046176520259182845), ('world', 0.0004610761574008715), ('dreary', 0.0004609253822412344), ('cause', 0.00045960466380283606), ('friend', 0.000459202212842113), ('play', 0.0004581810494076778), ('young', 0.0004568281095325631), ('hell', 0.0004556700448891434), ('(', 0.00045514303807208095), ('us', 0.00044837735223355175), (\"isn't\", 0.0004483457492441464), ('sometimes', 0.0004464445284101572), ('very', 0.0004451352203970871), ('sure', 0.0004430107884894134), ('bb18', 0.00044137678350978033), ('keep', 0.0004389968945567484), ('home', 0.0004383191738660237), ('juggle', 0.00043577419126212565), ('help', 0.0004324291309243572), ('10', 0.0004315033209956878), ('post', 0.0004291577686951405), ('person', 0.0004288536967984011), ('live', 0.00042828097900783884), ('punny', 0.00042626284924289465), ('talk', 0.00042422833065234786), ('dire', 0.000420789971819132), (\"they're\", 0.00042046816652592175), ('food', 0.00042015824549612333), ('pay', 0.0004195771069128216), ('üò≥', 0.00041892376612277327), ('quote', 0.0004172889855410426), ('pick', 0.00041654064806357917), ('boiling', 0.00041599673307915876), ('into', 0.0004140722707717301), ('cheerfulness', 0.00041283070397458527), ('terrorist', 0.00040806890327032094), ('melancholy', 0.0004073426374881649), ('pretty', 0.0004061736390070754), ('year', 0.0004054491835698218), ('every', 0.00040483795772312277), ('here', 0.0004035548267077199), ('while', 0.00040204313484677376), ('place', 0.00039870766078752424), ('god', 0.0003979685843288615), ('video', 0.00039622164134458626), ('india', 0.0003956000472133124), ('last', 0.0003954218448379637), ('around', 0.0003941147677853184), ('ask', 0.0003931806907276084), ('rejoicing', 0.0003918508893183145), (\"i'll\", 0.00039063628920463987), ('deject', 0.0003890963267404874), ('own', 0.0003886377352446857), ('full', 0.0003882337965580557), (\"there's\", 0.0003868056620562301), ('put', 0.0003853404385878216), ('news', 0.0003852597522393807), ('follow', 0.0003851885541355898), ('friday', 0.0003840376563827885), ('w', 0.0003831290633674341), ('eating', 0.0003831157068569139), ('least', 0.0003819655742576428), ('another', 0.0003819058274549316), ('wait', 0.00038171668216407863), ('shaking', 0.0003811293585954525), ('country', 0.0003796481553976362), ('intimidate', 0.00037822105129714056), (\"you're\", 0.0003780652828195721), ('t', 0.0003775025057043198), ('house', 0.00037730992114251214), (' ', 0.0003759082665989389), ('since', 0.0003724955473759273), ('awe', 0.0003710863804724714), ('aesthetically', 0.0003702943542675498), ('pessimism', 0.00036675493153718626), ('hard', 0.00036580919522417367), ('wish', 0.0003641273028644451), ('loss', 0.0003639120211330078), ('pak', 0.0003636388072262514), ('gbbo', 0.0003632223341001722), ('first', 0.00036286937039429167), ('damn', 0.0003625624418532156), ('intimidation', 0.00036199845161553006), ('affront', 0.00036148588090149925), ('growl', 0.00036121978846215643), ('everyone', 0.0003608623967441767), ('order', 0.00036021163698482723), ('interview', 0.0003581062003983056), ('bless', 0.0003550068128410708), ('*', 0.00035499762437402494), ('change', 0.00035484538583266287), ('big', 0.0003543298364547202), ('hopelessness', 0.0003538474242148175), ('name', 0.00035180610634313897), ('state', 0.00035115552166894645), ('inconsolable', 0.0003507566321855469), ('forget', 0.0003506541570395472), ('myself', 0.0003498592037741907), ('before', 0.00034889458078680156), ('dream', 0.0003481373224351578), ('watching', 0.0003473866699972107), ('jaunty', 0.0003424555086287785), ('nothing', 0.0003419580472958372), ('best', 0.0003403381620669547), ('affliction', 0.0003393876392417315), ('late', 0.0003385496954567962), ('leadership', 0.000337643938784835), ('shit', 0.00033690613060799685), (\"we're\", 0.0003367527427464052), ('feeling', 0.00033390856727787843), (\"won't\", 0.00033105020451182696), ('yet', 0.0003305858328235424), ('class', 0.00033018247222879414), ('death', 0.00033014278032591777), ('soul', 0.00032971990102201906), ('also', 0.0003290471931696713), ('clown', 0.00032496451858196943), ('these', 0.00032482749197333975), ('fight', 0.00032473397445978837), ('understand', 0.0003246444602583368), ('phone', 0.00032455308261790264), ('google', 0.000323769918994524), ('hour', 0.0003237430711568854), ('s', 0.0003221257704204445), ('shoot', 0.00032100856742834185), ('social', 0.0003200534846152025), ('awesome', 0.0003195312650769939), ('challenge', 0.0003181750026864235), ('moment', 0.0003176784315629599), ('two', 0.0003175470245980337), ('lovely', 0.0003173754163085681), ('die', 0.0003173468750468461), ('‚ò∫', 0.00031732488940623016), ('school', 0.00031675798477333745), (\"y'all\", 0.0003154632192378055), ('run', 0.0003149951812224202), ('bill', 0.0003145446855691455), ('heart', 0.0003115161430505074), ('warren', 0.00030982246602200784), ('open', 0.000308433970805729), ('anything', 0.00030783191265637067), ('sing', 0.00030587458226293376), ('already', 0.0003045760917972084), ('tho', 0.0003031347110903325)]\n"
     ]
    }
   ],
   "source": [
    "X = featured_dataset['valences'].values.tolist()\n",
    "y = featured_dataset['class'].values\n",
    "y[y == 'neutral'] = 'n'\n",
    "y[y == 'fear'] = 'f'\n",
    "y[y == 'anger'] = 'a'\n",
    "y[y == 'sadness'] = 's'\n",
    "y[y == 'joy'] = 'j'\n",
    "selected, mask = feature_selection(X, y, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id class                                             lemmas  \\\n",
      "0  1000001     n  [Check, this, video, out, -, -, President, Oba...   \n",
      "1  1000002     n  [need, suggestion, for, a, good, IR, filter, f...   \n",
      "2  1000003     n  [@user, :, I, just, check, my, google, for, my...   \n",
      "3  1000004     n  [be, in, San, Francisco, at, Bay, to, Breakers...   \n",
      "4  1000005     n                   [just, land, at, San, Francisco]   \n",
      "5  1000006     n     [San, Francisco, today, ., Any, suggestion, ?]   \n",
      "6  1000007     n  [On, my, way, to, see, Star, Trek, @, The, Esq...   \n",
      "7  1000008     n  [Going, to, see, star, trek, soon, with, my, d...   \n",
      "8  1000009     n  [Bill, Simmons, in, conversation, with, Malcol...   \n",
      "9  1000010     n         [play, with, cURL, and, the, Twitter, API]   \n",
      "\n",
      "                                            valences  \n",
      "0  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "9  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n"
     ]
    }
   ],
   "source": [
    "for index, row in featured_dataset.iterrows():\n",
    "    valences = np.array(row.valences[mask])\n",
    "    featured_dataset.set_value(index=index, col='valences', value=valences)\n",
    "print(featured_dataset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5348, 502)\n",
      "   v_0  v_1  v_2  v_3  v_4  v_5  v_6  v_7  v_8  v_9   ...     v_493  v_494  \\\n",
      "0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0   ...       0.0    0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   ...       0.0    0.0   \n",
      "2  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   ...       0.0    0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   ...       0.0    0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   ...       0.0    0.0   \n",
      "\n",
      "   v_495  v_496  v_497  v_498  v_499  v_500  v_501  emotion  \n",
      "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0        n  \n",
      "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0        n  \n",
      "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0        n  \n",
      "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0        n  \n",
      "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0        n  \n",
      "\n",
      "[5 rows x 503 columns]\n"
     ]
    }
   ],
   "source": [
    "temp = pd.DataFrame(np.vstack(featured_dataset.valences.values))\n",
    "print(np.vstack(featured_dataset.valences.values).shape)\n",
    "temp.columns = ['v_' + str(i) for i in range(len(selected))]\n",
    "temp['emotion'] = featured_dataset['class'].values\n",
    "print(temp.head(5))\n",
    "temp.to_csv('data_final/features_emotion_detection_tfidf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
