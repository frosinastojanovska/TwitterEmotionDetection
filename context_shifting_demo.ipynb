{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual valence shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lemmatization import lemmatize\n",
    "from lemmatization import pos_tagging\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from contextual_valence_shifting import ContextualValenceShifter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: None\n",
      "Position: 0\n",
      "Relation: None\n",
      "Children: [5]\n",
      "Word: The\n",
      "Position: 1\n",
      "Relation: det\n",
      "Children: []\n",
      "Word: organizer\n",
      "Position: 4\n",
      "Relation: nsubj\n",
      "Children: [1, 3]\n",
      "Word: very\n",
      "Position: 2\n",
      "Relation: advmod\n",
      "Children: []\n",
      "Word: brilliant\n",
      "Position: 3\n",
      "Relation: amod\n",
      "Children: [2]\n",
      "Word: failed\n",
      "Position: 5\n",
      "Relation: root\n",
      "Children: [4, 7]\n",
      "Word: to\n",
      "Position: 6\n",
      "Relation: mark\n",
      "Children: []\n",
      "Word: solve\n",
      "Position: 7\n",
      "Relation: xcomp\n",
      "Children: [6, 9]\n",
      "Word: the\n",
      "Position: 8\n",
      "Relation: det\n",
      "Children: []\n",
      "Word: problem\n",
      "Position: 9\n",
      "Relation: dobj\n",
      "Children: [8]\n",
      "Triples: [(('failed', 'VBD'), 'nsubj', ('organizer', 'NN')), (('organizer', 'NN'), 'det', ('The', 'DT')), (('organizer', 'NN'), 'amod', ('brilliant', 'JJ')), (('brilliant', 'JJ'), 'advmod', ('very', 'RB')), (('failed', 'VBD'), 'xcomp', ('solve', 'VB')), (('solve', 'VB'), 'mark', ('to', 'TO')), (('solve', 'VB'), 'dobj', ('problem', 'NN')), (('problem', 'NN'), 'det', ('the', 'DT'))]\n",
      "Tree: [Tree('organizer', ['The', Tree('brilliant', ['very'])]), Tree('solve', ['to', Tree('problem', ['the'])])]\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse import DependencyGraph\n",
    "path_to_jar = 'stanford_parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'stanford_parser/stanford-parser-3.9.1-models.jar'\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "result = dependency_parser.parse(['The', 'very', 'brilliant', 'organizer', 'failed', 'to', 'solve', 'the', 'problem'])\n",
    "dg = result.__next__()\n",
    "for node in dg.nodes.values():\n",
    "    print('Word:', node['word'])\n",
    "    print('Position:', node['address'])\n",
    "    print('Relation:', node['rel'])\n",
    "    print('Children:', list(chain.from_iterable(node['deps'].values())))\n",
    "print('Triples:', list(dg.triples()))\n",
    "print('Tree:', list(dg.tree()))\n",
    "\n",
    "tree = dg.tree()\n",
    "from nltk.draw.tree import draw_trees\n",
    "draw_trees(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word  valence  arousal\n",
      "1     aardvark     6.26     2.41\n",
      "2      abalone     5.30     2.65\n",
      "3      abandon     2.84     3.73\n",
      "4  abandonment     2.63     4.95\n",
      "5        abbey     5.85     2.20\n"
     ]
    }
   ],
   "source": [
    "lexicon = pd.read_csv('lexicons/Ratings_Warriner_et_al.csv', usecols=[0, 1, 2, 5], index_col=0)\n",
    "lexicon.columns = ['word', 'valence', 'arousal']\n",
    "print(lexicon.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valence shifting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_jar = 'stanford_parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'stanford_parser/stanford-parser-3.9.1-models.jar'\n",
    "valence_shifter = ContextualValenceShifter(path_to_jar, path_to_models_jar, lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Negation rule\n",
    "If a word is in relation with negatives (e.g. not, never, nothing), then the initial valence of the word is shifted, i.e. is multiplied by -1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1.6799999999999997, 0, -1.6600000000000001]\n",
      "[0, 1.6799999999999997, 0, 1.6600000000000001]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'He is not stupid'\n",
    "tknzr = TweetTokenizer()\n",
    "tokens = tknzr.tokenize(sentence)\n",
    "tags = pos_tagging(tokens)\n",
    "lemmas = [x[1] for x in lemmatize(tags)]\n",
    "\n",
    "intial_valences = valence_shifter.get_initial_valences_sentence(lemmas)\n",
    "print(intial_valences)\n",
    "valences = valence_shifter.change_valence_sentence(lemmas, intial_valences)\n",
    "print(valences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Intensifiers rule\n",
    " If there is an intensifier in the tweet, then the valence of the word that is in relation with the intensifier is increased by multiplying the initial valence with 1,5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1.83, 1.6799999999999997, 0, -2.22]\n",
      "[0, -1.83, -1.6799999999999997, 0, -3.33]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The film was absolutely awful'\n",
    "tknzr = TweetTokenizer()\n",
    "tokens = tknzr.tokenize(sentence)\n",
    "tags = pos_tagging(tokens)\n",
    "lemmas = [x[1] for x in lemmatize(tags)]\n",
    "\n",
    "intial_valences = valence_shifter.get_initial_valences_sentence(lemmas)\n",
    "print(intial_valences)\n",
    "valences = valence_shifter.change_valence_sentence(lemmas, intial_valences)\n",
    "print(valences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Mitigators rule\n",
    "If there is a mitigator in the tweet, then the valence of the word that is in relation with the mitigator is decreased by multiplying the initial valence with 0,5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0.23000000000000043, 0, 0, 1.8600000000000003, 0, 1.6799999999999997, 0, -0.20999999999999996]\n",
      "[0, 0, -0.23000000000000043, 0, 0, -1.8600000000000003, 0, -1.6799999999999997, 0, -0.10499999999999998]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'By the end of the day we were rather tired'\n",
    "tknzr = TweetTokenizer()\n",
    "tokens = tknzr.tokenize(sentence)\n",
    "tags = pos_tagging(tokens)\n",
    "lemmas = [x[1] for x in lemmatize(tags)]\n",
    "\n",
    "intial_valences = valence_shifter.get_initial_valences_sentence(lemmas)\n",
    "print(intial_valences)\n",
    "valences = valence_shifter.change_valence_sentence(lemmas, intial_valences)\n",
    "print(valences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Negative words rule  \n",
    "If the word is in a relation with a negative word, then it is multiplicated with -1 only if the word has positive valence. Otherwise, the valence remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 3.0, 1.8499999999999996, -2.17, 0, 1.7999999999999998, 0, -0.98]\n",
      "[0, 0, -4.5, -1.8499999999999996, -2.17, 0, -1.7999999999999998, 0, -0.98]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The very brilliant organizer failed to solve the problem'\n",
    "tknzr = TweetTokenizer()\n",
    "tokens = tknzr.tokenize(sentence)\n",
    "tags = pos_tagging(tokens)\n",
    "lemmas = [x[1] for x in lemmatize(tags)]\n",
    "\n",
    "intial_valences = valence_shifter.get_initial_valences_sentence(lemmas)\n",
    "print(intial_valences)\n",
    "valences = valence_shifter.change_valence_sentence(lemmas, intial_valences)\n",
    "print(valences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Conjuctive adverbs\n",
    "If there is a conjunctive adverb in the tweet, then the valences are neutralized by multiplication with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1.6799999999999997, 3.0, 0, -0.04999999999999982, 0, 0, 1.6799999999999997, 0, -2.17, 2.87]\n",
      "[0, 0, 1.6799999999999997, -0.0, 0, -0.04999999999999982, 0, 0, 1.6799999999999997, 0, -2.17, -2.87]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Although Boris is brilliant at math, he is a horrible teacher'\n",
    "tknzr = TweetTokenizer()\n",
    "tokens = tknzr.tokenize(sentence)\n",
    "tags = pos_tagging(tokens)\n",
    "lemmas = [x[1] for x in lemmatize(tags)]\n",
    "\n",
    "intial_valences = valence_shifter.get_initial_valences_sentence(lemmas)\n",
    "print(intial_valences)\n",
    "valences = valence_shifter.change_valence_sentence(lemmas, intial_valences)\n",
    "print(valences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"wouldn't\", 'wish', 'anxiety', 'and', 'depression', 'even', 'on', 'the', 'worst', 'of', 'people', '.', \"It's\", 'not', 'fun', '.', '#anxiety', '#depression']\n",
      "[0, 0, 2.6399999999999997, -2.12, 0, -2.06, 0, 0, 0, -1.2599999999999998, 0, 1.2000000000000002, 0, 0, 0, 3.869999999999999, 0, -2.12, -2.06]\n",
      "[0, 0, -2.6399999999999997, -2.12, 0, -2.06, 0, 0, 0, -1.2599999999999998, 0, -1.2000000000000002, 0, 0, 0, 3.869999999999999, 0, -2.12, -2.06]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I wouldn't wish anxiety and depression even on the worst of people. It's not fun. #anxiety #depression\"\n",
    "tknzr = TweetTokenizer()\n",
    "tokens = tknzr.tokenize(sentence)\n",
    "print(tokens)\n",
    "tags = pos_tagging(tokens)\n",
    "lemmas = [x[1] for x in lemmatize(tags)]\n",
    "\n",
    "intial_valences = valence_shifter.get_initial_valences_sentence(lemmas)\n",
    "print(intial_valences)\n",
    "valences = valence_shifter.change_valence_sentence(lemmas, intial_valences)\n",
    "print(valences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
