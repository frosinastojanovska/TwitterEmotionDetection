{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion detection from tweets (lexicon + word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from feature_extraction import FeatureExtractionContextValenceShifting\n",
    "from feature_selection import generate_initial_features, feature_selection \n",
    "from preprocessing import fix_encoding, split_tweet_sentences, tokenize_tweets, get_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and preprocess dataset\n",
    "\n",
    "Load the dataset contaning the tweets and their emotions. For text preprocessing first the tweet text is encoded (fixed), then the tweet is tokenized, and finally the tokens are lematized. The dataset needs to have a column with name tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id    class                                             tokens  \\\n",
      "0  1000001  neutral  [[Check, this, video, out, -, -, President, Ob...   \n",
      "1  1000002  neutral  [[need, suggestions, for, a, good, IR, filter,...   \n",
      "2  1000003  neutral  [[@surfit, :, I, just, checked, my, google, fo...   \n",
      "3  1000004  neutral  [[is, in, San, Francisco, at, Bay, to, Breaker...   \n",
      "4  1000005  neutral               [[just, landed, at, San, Francisco]]   \n",
      "5  1000006  neutral  [[San, Francisco, today, .], [Any, suggestions...   \n",
      "6  1000007  neutral  [[On, my, way, to, see, Star, Trek, @, The, Es...   \n",
      "7  1000008  neutral  [[Going, to, see, star, trek, soon, with, my, ...   \n",
      "8  1000009  neutral  [[Bill, Simmons, in, conversation, with, Malco...   \n",
      "9  1000010  neutral    [[playing, with, cURL, and, the, Twitter, API]]   \n",
      "\n",
      "                                              lemmas  \n",
      "0  [[Check, this, video, out, -, -, President, Ob...  \n",
      "1  [[need, suggestion, for, a, good, IR, filter, ...  \n",
      "2  [[@user, :, I, just, check, my, google, for, m...  \n",
      "3  [[be, in, San, Francisco, at, Bay, to, Breaker...  \n",
      "4                 [[just, land, at, San, Francisco]]  \n",
      "5  [[San, Francisco, today, .], [Any, suggestion,...  \n",
      "6  [[On, my, way, to, see, Star, Trek, @, The, Es...  \n",
      "7  [[Going, to, see, star, trek, soon, with, my, ...  \n",
      "8  [[Bill, Simmons, in, conversation, with, Malco...  \n",
      "9       [[play, with, cURL, and, the, Twitter, API]]  \n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_excel('data/merged_datasets.xlsx')\n",
    "# fix the tweet text\n",
    "dataset = fix_encoding(dataset)\n",
    "# split the tweet text into sentences\n",
    "dataset = split_tweet_sentences(dataset)\n",
    "# tokenize each sentence of the tweets\n",
    "dataset = tokenize_tweets(dataset)\n",
    "# lemmatise the tweets\n",
    "dataset = get_lemmas(dataset)\n",
    "\n",
    "dataset = dataset.drop(['emotion_intensity', 'tweet'], axis=1)\n",
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load lexicon and word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = pd.read_csv('lexicons/Ratings_Warriner_et_al.csv', usecols=[0, 1, 2, 5], index_col=0)\n",
    "lexicon.columns = ['word', 'valence', 'arousal']\n",
    "model = KeyedVectors.load_word2vec_format('glove.twitter.27B.200d.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_jar = 'stanford_parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'stanford_parser/stanford-parser-3.9.1-models.jar'\n",
    "valence_shifter = FeatureExtractionContextValenceShifting(path_to_jar, path_to_models_jar, lexicon, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set intial valences from lexicon\n",
    "dataset = valence_shifter.get_initial_valences(dataset)\n",
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_dataset, vocab = generate_initial_features(dataset)\n",
    "print(featured_dataset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = featured_dataset['valences'].values.tolist()\n",
    "y = featured_dataset['class'].values\n",
    "y[y == 'neutral'] = 'n'\n",
    "y[y == 'fear'] = 'f'\n",
    "y[y == 'anger'] = 'a'\n",
    "y[y == 'sadness'] = 's'\n",
    "y[y == 'joy'] = 'j'\n",
    "selected, mask = feature_selection(X, y, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in featured_dataset.iterrows():\n",
    "    valences = np.array(row.valences[mask])\n",
    "    featured_dataset.set_value(index=index, col='valences', value=valences)\n",
    "print(featured_dataset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(np.vstack(featured_dataset.valences.values))\n",
    "print(np.vstack(featured_dataset.valences.values).shape)\n",
    "temp.columns = ['v_' + str(i) for i in range(len(selected))]\n",
    "temp['emotion'] = featured_dataset['class'].values\n",
    "print(temp.head(5))\n",
    "temp.to_csv('data_final/features_emotion_detection_w2v.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
